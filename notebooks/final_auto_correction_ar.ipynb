{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUt6sgNTzRLA",
        "outputId": "c85a4a16-b308-4162-d087-284aba5f348e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import  packeges"
      ],
      "metadata": {
        "id": "p3tfITnpLOD2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "j-QHM5u5JFGr"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AdamW\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.nn.functional import softmax\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read File"
      ],
      "metadata": {
        "id": "XyyvO5B8NXGB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wuyMLN1bJFGs"
      },
      "outputs": [],
      "source": [
        "# Read the Arabic file\n",
        "with open(\"/content/drive/My Drive/ara-eg_newscrawl-OSIAN_2018_10K-sentences.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    sentences = file.readlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# modification function\n",
        "##### to simulate types of spelling mistakes"
      ],
      "metadata": {
        "id": "qYp1ZCtsNcZo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHLaYvcxJFGr"
      },
      "outputs": [],
      "source": [
        "def uncorrect_word(word, word_list):\n",
        "    modification_type = random.choice([\"change_letter\", \"replace_word\", \"swap_letters\", \"add_letter\"])\n",
        "\n",
        "    if len(word) <= 1:\n",
        "        return word\n",
        "\n",
        "\n",
        "    if modification_type == \"change_letter\":\n",
        "        position = random.randint(0, len(word) - 1)\n",
        "        new_letter = random.choice([letter for letter in \"ابتثجحخدذرزسشصضطظعغفقكلمنهوي\" if letter != word[position]])\n",
        "        modified_word = word[:position] + new_letter + word[position + 1:]\n",
        "\n",
        "    elif modification_type == \"replace_word\":\n",
        "        modified_word = random.choice(word_list)\n",
        "        while modified_word == word:\n",
        "            modified_word = random.choice(word_list)\n",
        "\n",
        "    elif modification_type == \"swap_letters\":\n",
        "\n",
        "        position1 = random.randint(0, len(word) - 1)\n",
        "        position2 = random.randint(0, len(word) - 1)\n",
        "        word_list = list(word)\n",
        "        word_list[position1], word_list[position2] = word_list[position2], word_list[position1]\n",
        "        modified_word = ''.join(word_list)\n",
        "\n",
        "    elif modification_type == \"add_letter\":\n",
        "        position = random.randint(0, len(word))\n",
        "        new_letter = random.choice([letter for letter in \"ابتثجحخدذرزسشصضطظعغفقكلمنهوي\"])\n",
        "        modified_word = word[:position] + new_letter + word[position:]\n",
        "\n",
        "    return modified_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2ZByTkrJFGt"
      },
      "outputs": [],
      "source": [
        "# create spelling mistakes file (labels file)\n",
        "\n",
        "for index, sentence in enumerate(sentences):\n",
        "    words = sentence.strip().split()\n",
        "    word_index = random.randint(0, len(words) - 1)\n",
        "    word_to_uncorrect = words[word_index]\n",
        "    uncorrected_word = uncorrect_word(word_to_uncorrect, words)\n",
        "    words[word_index] = uncorrected_word\n",
        "    modified_sentence = ' '.join(words)\n",
        "\n",
        "     # Write the modified sentence back to the file\n",
        "    with open(\"ara-eg_newscrawl-OSIAN_2018_10K/ara-eg_newscrawl-OSIAN_2018_10K-labels.txt\", \"a\", encoding=\"utf-8\") as modified_file:\n",
        "        modified_file.write(modified_sentence.strip() + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing data"
      ],
      "metadata": {
        "id": "Hij0A60bN3Za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove noisy data"
      ],
      "metadata": {
        "id": "5kz1o3rVN88i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lZmTm-tbzsG3"
      },
      "outputs": [],
      "source": [
        "def clean_sentence(sentence):\n",
        "    # Remove leading digits and any whitespace\n",
        "    cleaned_sentence = re.sub(r'^\\d+\\s*', '', sentence)\n",
        "    # Remove any leading or trailing whitespace\n",
        "    cleaned_sentence = cleaned_sentence.strip()\n",
        "    return cleaned_sentence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Read Files and store them in list"
      ],
      "metadata": {
        "id": "YwJB6E6qOKLz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VczFPzZGzsNc"
      },
      "outputs": [],
      "source": [
        "# Read the file and store sentences in a list\n",
        "y = []\n",
        "with open(\"/content/drive/My Drive/ara-eg_newscrawl-OSIAN_2018_10K-sentences.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "        cleaned_line = clean_sentence(line)\n",
        "        if cleaned_line:\n",
        "            y.append(cleaned_line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yktBZm9DzsPi"
      },
      "outputs": [],
      "source": [
        "\n",
        "x = []\n",
        "with open(\"/content/drive/My Drive/ara-eg_newscrawl-OSIAN_2018_10K-labels.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "        cleaned_line = clean_sentence(line)\n",
        "        if cleaned_line:\n",
        "            x.append(cleaned_line)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### remove noisy data"
      ],
      "metadata": {
        "id": "s6c3KkrwOZGS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FZrH-FcGzsRv"
      },
      "outputs": [],
      "source": [
        "def clean_sentence(sentence):\n",
        "    # Remove English words using regular expression\n",
        "    cleaned_sentence = re.sub(r'\\b[a-zA-Z]+\\b', '', sentence)\n",
        "    # Remove numbers and signs using regular expression\n",
        "    cleaned_sentence = re.sub(r'\\d+|\\W+', ' ', cleaned_sentence)\n",
        "    # Remove punctuation marks\n",
        "    cleaned_sentence = re.sub(r'[^\\w\\s]', '', cleaned_sentence)\n",
        "    # Normalize diacritics (fatha, kasra, damma)\n",
        "    cleaned_sentence = cleaned_sentence.replace('ً', '').replace('ٌ', '').replace('ٍ', '').replace('َ', '').replace('ُ', '').replace('ِ', '')\n",
        "    # Remove special characters\n",
        "    cleaned_sentence= re.sub(r'[^\\u0600-\\u06FF\\s\\d]', '', cleaned_sentence)\n",
        "    # Remove numbers\n",
        "    cleaned_sentence= re.sub(r'\\d+', '', cleaned_sentence)\n",
        "    # Normalize whitespace\n",
        "    cleaned_sentence= re.sub(r'\\s+', ' ', cleaned_sentence.strip())\n",
        "    # Remove extra whitespace\n",
        "    cleaned_sentence = ' '.join(cleaned_sentence.split())\n",
        "    return cleaned_sentence\n",
        "\n",
        "\n",
        "x = [clean_sentence(sentence) for sentence in x]\n",
        "\n",
        "y = [clean_sentence(sentence) for sentence in y]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44bq514OzsTm",
        "outputId": "64f5ed80-0861-449f-e28c-2e713ca84205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['مليارات و مليون درهم خلال تكل الفترة', 'معلومات خاطئة نعتقد للأسنان جيدة للأسنان معلومات خاطئة نعتقد أنها جيدة للأسنان', 'مسلسل يونس ولد بطولة عمرو سعد وسيعرض على قناة مصر دراما المستقبل السومرية', 'الدكتورة منى بنت عبدالله بن سعيد آل مشيط', 'صودا الخبز هو معجون طبيعي للأسنان حثي ينصح بخلط ربع ملعقة صغيرة من صودا الخبز مع الماء وغسل الأسنان ببه']\n",
            "['مليارات و مليون درهم خلال تلك الفترة', 'معلومات خاطئة نعتقد أنها جيدة للأسنان معلومات خاطئة نعتقد أنها جيدة للأسنان', 'مسلسل يونس ولد فضة بطولة عمرو سعد وسيعرض على قناة مصر دراما المستقبل السومرية', 'الدكتورة منى بنت عبدالله بن سعيد آل مشيط', 'صودا الخبز هو معجون طبيعي للأسنان حيث ينصح بخلط ربع ملعقة صغيرة من صودا الخبز مع الماء وغسل الأسنان ببه']\n"
          ]
        }
      ],
      "source": [
        "# show some of data\n",
        "print(x[:5])\n",
        "print(y[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation and Model Training"
      ],
      "metadata": {
        "id": "Xy49jsfxOi5S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iAEnCcKJFGv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the pre-trained AraBERT model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabert\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"aubmindlab/bert-base-arabert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "j4eAeiLeJFGw"
      },
      "outputs": [],
      "source": [
        "max_length = 32\n",
        "# Tokenize the sentences\n",
        "tokenized_x = tokenizer(x, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "tokenized_y = tokenizer(y, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jHXy9f4aJFGw"
      },
      "outputs": [],
      "source": [
        "# Convert tokenized sentences to PyTorch tensors\n",
        "input_ids = tokenized_x.input_ids\n",
        "labels = tokenized_y.input_ids\n",
        "\n",
        "# Create a PyTorch Dataset\n",
        "dataset = TensorDataset(input_ids, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GMiVnK-GJFGx"
      },
      "outputs": [],
      "source": [
        "# Define training parameters\n",
        "learning_rate = 5e-5\n",
        "num_epochs = 5\n",
        "batch_size = 8\n",
        "num_folds = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Uj03If8TJFGx"
      },
      "outputs": [],
      "source": [
        "# Initialize KFold\n",
        "kf = KFold(n_splits=num_folds, shuffle=True)\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "test_accuracies = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRVkzrqz0t--",
        "outputId": "359ef222-1a5e-411a-b15c-25c1a01094a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1, Epoch 1, Train Loss: 1.5150324387550353, Train Accuracy: 0.728828125\n",
            "Fold 1, Epoch 2, Train Loss: 1.0375603317320348, Train Accuracy: 0.77244140625\n",
            "Fold 1, Epoch 3, Train Loss: 0.7404165369048714, Train Accuracy: 0.82978515625\n",
            "Fold 1, Epoch 4, Train Loss: 0.5283033225052058, Train Accuracy: 0.87189453125\n",
            "Fold 1, Epoch 5, Train Loss: 0.39420810176059606, Train Accuracy: 0.89934375\n",
            "Fold 1, Test Loss: 1.441913595199585, Test Accuracy: 0.79703125\n",
            "Fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2, Epoch 1, Train Loss: 0.5768548252135515, Train Accuracy: 0.8749296875\n",
            "Fold 2, Epoch 2, Train Loss: 0.3409107153499499, Train Accuracy: 0.9164765625\n",
            "Fold 2, Epoch 3, Train Loss: 0.25301454787235705, Train Accuracy: 0.93281640625\n",
            "Fold 2, Epoch 4, Train Loss: 0.20617747255042196, Train Accuracy: 0.9429921875\n",
            "Fold 2, Epoch 5, Train Loss: 0.18161973569821566, Train Accuracy: 0.94958984375\n",
            "Fold 2, Test Loss: 0.355958178024739, Test Accuracy: 0.919796875\n",
            "Fold 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3, Epoch 1, Train Loss: 0.2243199434815906, Train Accuracy: 0.9429140625\n",
            "Fold 3, Epoch 2, Train Loss: 0.15198227676935494, Train Accuracy: 0.95836328125\n",
            "Fold 3, Epoch 3, Train Loss: 0.11981610896391794, Train Accuracy: 0.966875\n",
            "Fold 3, Epoch 4, Train Loss: 0.08938672306574881, Train Accuracy: 0.9748828125\n",
            "Fold 3, Epoch 5, Train Loss: 0.08262216855539009, Train Accuracy: 0.97708203125\n",
            "Fold 3, Test Loss: 0.14384007084788755, Test Accuracy: 0.962671875\n",
            "Fold 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4, Epoch 1, Train Loss: 0.11850827761809342, Train Accuracy: 0.96832421875\n",
            "Fold 4, Epoch 2, Train Loss: 0.08248254560399801, Train Accuracy: 0.9775703125\n",
            "Fold 4, Epoch 3, Train Loss: 0.05369452366384212, Train Accuracy: 0.98499609375\n",
            "Fold 4, Epoch 4, Train Loss: 0.06879480622487609, Train Accuracy: 0.98235546875\n",
            "Fold 4, Epoch 5, Train Loss: 0.053868913089332636, Train Accuracy: 0.98588671875\n",
            "Fold 4, Test Loss: 0.13480126849142834, Test Accuracy: 0.96490625\n",
            "Fold 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5, Epoch 1, Train Loss: 0.07498310002079234, Train Accuracy: 0.9806328125\n",
            "Fold 5, Epoch 2, Train Loss: 0.06652058501041029, Train Accuracy: 0.98307421875\n",
            "Fold 5, Epoch 3, Train Loss: 0.05092586832679808, Train Accuracy: 0.98691796875\n",
            "Fold 5, Epoch 4, Train Loss: 0.03917282380303368, Train Accuracy: 0.98996875\n",
            "Fold 5, Epoch 5, Train Loss: 0.037189616818795915, Train Accuracy: 0.99064453125\n",
            "Fold 5, Test Loss: 0.037265214257058685, Test Accuracy: 0.99053125\n",
            "Average Test Loss: 0.42275566536413967, Average Test Accuracy: 0.9269874999999999\n"
          ]
        }
      ],
      "source": [
        "# Cross-validation loop\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(dataset)):\n",
        "    print(f\"Fold {fold+1}/{num_folds}\")\n",
        "\n",
        "    # Split dataset into train and test sets for this fold\n",
        "    train_dataset = torch.utils.data.Subset(dataset, train_index)\n",
        "    test_dataset = torch.utils.data.Subset(dataset, test_index)\n",
        "\n",
        "    # Create DataLoaders for train and test sets\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Set device to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define optimizer and loss function\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train the model for this fold\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_preds = []\n",
        "        train_labels = []\n",
        "        for batch in train_dataloader:\n",
        "            # Move batch to device\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, labels = batch\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Calculate predictions and true labels\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            train_preds.extend(preds.cpu().numpy())\n",
        "            train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Convert the train_labels and train_preds lists to NumPy arrays\n",
        "        train_labels = np.array(train_labels)\n",
        "        train_preds = np.array(train_preds)\n",
        "\n",
        "        # Calculate average training loss for the epoch\n",
        "        train_avg_loss = train_loss / len(train_dataloader)\n",
        "\n",
        "        # Calculate the accuracy score\n",
        "        train_accuracy = accuracy_score(train_labels.ravel(), train_preds.ravel())\n",
        "        print(f\"Fold {fold+1}, Epoch {epoch+1}, Train Loss: {train_avg_loss}, Train Accuracy: {train_accuracy}\")\n",
        "\n",
        "    # Test for this fold\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_preds = []\n",
        "    test_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, labels = batch\n",
        "\n",
        "            outputs = model(input_ids=input_ids, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate predictions and true labels\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            test_preds.extend(preds.cpu().numpy())\n",
        "            test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Convert the test_labels and test_preds lists to NumPy arrays\n",
        "    test_labels = np.array(test_labels)\n",
        "    test_preds = np.array(test_preds)\n",
        "\n",
        "    # Calculate average test loss for the fold\n",
        "    test_avg_loss = test_loss / len(test_dataloader)\n",
        "\n",
        "    # Calculate the accuracy score\n",
        "    test_accuracy = accuracy_score(test_labels.ravel(), test_preds.ravel())\n",
        "    print(f\"Fold {fold+1}, Test Loss: {test_avg_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "    # Append metrics for this fold to the lists\n",
        "    test_losses.append(test_avg_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "# Print average test loss and accuracy across all folds\n",
        "print(f\"Average Test Loss: {np.mean(test_losses)}, Average Test Accuracy: {np.mean(test_accuracies)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### correct sentences based on the predictions of a pre-trained language model"
      ],
      "metadata": {
        "id": "o5p3qmgPO-9e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyu12gfS1HjD",
        "outputId": "96632a4f-100d-4eaa-badf-c3ab33f172ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: ['مليارات و مليون درهم خلال تكل الفترة', 'معلومات خاطئة نعتقد للأسنان جيدة للأسنان معلومات خاطئة نعتقد أنها جيدة للأسنان', 'مسلسل يونس ولد بطولة عمرو سعد وسيعرض على قناة مصر دراما المستقبل السومرية', 'الدكتورة منى بنت عبدالله بن سعيد آل مشيط', 'صودا الخبز هو معجون طبيعي للأسنان حثي ينصح بخلط ربع ملعقة صغيرة من صودا الخبز مع الماء وغسل الأسنان بب']\n",
            "Corrected sentence: ['مليارات و مليون درهم خلال تلك الفترة', 'معلومات خاطئة نعتقد أنها جيدة للأسنان معلومات خاطئة نعتقد أنها جيدة للأسنانناتن', 'مسلسل يونس ولد فضة بطولة عمرو سعد وسيعرض على قناة مصر دراما السوم السوم', 'الدكتورة منى بنت عبدالله بن سعيد آل مشيط', 'صودا الخبز هو معجون طبيعي للأسنان حيث ينصح بخلط ربع ملعقة صغيرة من صودا الخبز مع الماء وحمر الأسنان بب ال']\n"
          ]
        }
      ],
      "source": [
        "def auto_correct_sentence(model, tokenizer, sentence, device):\n",
        "\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # Move inputs to device\n",
        "    inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
        "\n",
        "    # Forward pass through the model\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Get probabilities using softmax\n",
        "    probabilities = softmax(logits, dim=2)\n",
        "\n",
        "    # Get the predicted labels\n",
        "    _, predicted_labels = torch.max(probabilities, dim=2)\n",
        "\n",
        "    # Decode predicted labels to tokens\n",
        "    corrected_tokens = [tokenizer.decode(tokens, skip_special_tokens=True) for tokens in predicted_labels]\n",
        "\n",
        "    # Join tokens to form corrected sentence\n",
        "    corrected_sentence = \" \".join(corrected_tokens)\n",
        "\n",
        "    return corrected_sentence\n",
        "\n",
        "# Example usage\n",
        "def auto_correct_sentences(model, tokenizer, sentences, device):\n",
        "    corrected_sentences = []\n",
        "    for sentence in sentences:\n",
        "        corrected_sentence = auto_correct_sentence(model, tokenizer, sentence, device)\n",
        "        corrected_sentences.append(corrected_sentence)\n",
        "    return corrected_sentences\n",
        "\n",
        "# Example usage\n",
        "sentences = ['مليارات و مليون درهم خلال تكل الفترة', 'معلومات خاطئة نعتقد للأسنان جيدة للأسنان معلومات خاطئة نعتقد أنها جيدة للأسنان', 'مسلسل يونس ولد بطولة عمرو سعد وسيعرض على قناة مصر دراما المستقبل السومرية', 'الدكتورة منى بنت عبدالله بن سعيد آل مشيط', 'صودا الخبز هو معجون طبيعي للأسنان حثي ينصح بخلط ربع ملعقة صغيرة من صودا الخبز مع الماء وغسل الأسنان بب']\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "corrected_sentences = auto_correct_sentences(model, tokenizer, sentences, device)\n",
        "print(\"Original sentence:\", sentences[:5])\n",
        "print(\"Corrected sentence:\", corrected_sentences[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8zhtvKUF3Wj"
      },
      "outputs": [],
      "source": [
        "# Cross-validation loop\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(dataset)):\n",
        "    print(f\"Fold {fold+1}/{num_folds}\")\n",
        "\n",
        "    # Split dataset into train and test sets for this fold\n",
        "    train_dataset = torch.utils.data.Subset(dataset, train_index)\n",
        "    test_dataset = torch.utils.data.Subset(dataset, test_index)\n",
        "\n",
        "    # Create DataLoaders for train and test sets\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Set device to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define optimizer and loss function\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train the model for this fold\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_preds = []\n",
        "        train_labels = []\n",
        "        for batch in train_dataloader:\n",
        "            # Move batch to device\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, labels = batch\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "    # Evaluate the model on the test set for this fold\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_preds = []\n",
        "    test_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, labels = batch\n",
        "\n",
        "            outputs = model(input_ids=input_ids, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate predictions and true labels for accuracy calculation\n",
        "            logits = outputs.logits\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            test_preds.extend(preds.cpu().numpy())\n",
        "            test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Convert the test_labels and test_preds lists to NumPy arrays\n",
        "    test_labels = np.array(test_labels)\n",
        "    test_preds = np.array(test_preds)\n",
        "\n",
        "    # Calculate average test loss for the fold\n",
        "    test_avg_loss = test_loss / len(test_dataloader)\n",
        "    test_losses.append(test_avg_loss)\n",
        "\n",
        "    # Calculate accuracy for the fold\n",
        "    test_accuracy = accuracy_score(test_labels.ravel(), test_preds.ravel())\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_folds + 1), test_losses, label='Test Loss')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Test Loss Across Folds')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_folds + 1), test_accuracies, label='Test Accuracy')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test Accuracy Across Folds')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}